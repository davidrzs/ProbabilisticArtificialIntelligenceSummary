\documentclass[11pt]{article}
\usepackage[headings]{fullpage}
\usepackage[landscape,paper=a4paper,margin=0.4cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{color}
\usepackage[dvipsnames]{xcolor}

\usepackage{mathpazo}

% Text formatting
\usepackage{parskip}
\setlength{\parskip}{2pt} %set space after paragraph

\usepackage{etoolbox}
\usepackage{titlesec}
\titlespacing*{\section}{0pt}{0pt}{0pt}

\titlespacing*{\subsection}{0pt}{-1pt}{-1pt}
\titleformat*{\section}{\normalfont\bfseries\large\color{Blue}}
\titlespacing*{\subsection}{0pt}{0pt}{0pt}
\titleformat*{\subsection}{\normalfont\large\bfseries\itshape}
\titlespacing*{\subsection}{0pt}{0pt}{0pt}
\titleformat*{\subsubsection}{\normalfont\large\itshape}
\newcommand{\gauss}{\mathcal{N}}
\newcommand{\argmin}{\operatorname{argmin}}
\newcommand{\argmax}{\operatorname{argmax}}
\newcommand{\E}{\mathbb{E}}

\usepackage{mdframed}
\mdfsetup{
	innerleftmargin=0cm,innertopmargin=2pt,innerbottommargin=2pt%
	linecolor=blue,
	rightline=false,bottomline=true,
	leftline=false,
	skipbelow=0pt,
	skipabove=0pt}



\usepackage[all=tight,extreme]{savetrees}
\usepackage{multicol}

\begin{document}
	
	\begin{multicols*}{3}
	\section*{Bayesian Linear Regression (BLR)}
	\begin{itemize}
		\item \textit{Prior:} $p(w) = \gauss (0,I)$
\item \textit{Likelihood:} $p(y|x,w,\sigma_n) = \gauss (y;w^Tx,\sigma^2_{n})$ where $y = w^Tx_i + \epsilon_i$ with $\epsilon_i \sim \gauss(0,\sigma^2)$.
\item \textit{Posterior:} $p(w|X,y) = \gauss(w;\bar{\mu},\bar{\Sigma})$ with $\bar{\mu} = (X^TX + \sigma^2 I )^{-1}X^Ty$ and $\bar{\Sigma} = (\sigma_n^{-2}X^T X + I)^{-1}$.
\item \textit{Predictions:} $ p(y^* | x^*, x_{1:n}, y_{1:n}) \int p(y^*|x^*, w) p(w | x_{1:n}, y_{1:n}) dw$ where $f^* = w^T x^*$
\begin{itemize}
	\item $p(f^*| X,y,x^*) = \gauss(\bar{\mu}^T x^*, x^{*T}\bar{\Sigma} x^*)$
	\item $p(y^*| X,y,x^*) = \gauss(\bar{\mu}^T x^*, x^{*T}\bar{\Sigma} x^* + \sigma_n^2)$
\end{itemize}
	\end{itemize}
	
	TODO: types of uncertainty.
	
	\section*{General Bayesian Learning}
	\begin{itemize}
		\item \textit{Prior:} $p(\theta)$
		\item \textit{Likelihood:} $p(y_{1:n}| x_{1:n}, \theta) = \prod_{i=1}^{n} p(y_i | x_i, \theta)$
		\item \textit{Posterior:} $p(\theta | x_{1:n}, y_{1:n}) = \frac{1}{Z} p(\theta) \prod_{i=1}^{n} p(y_i | x_i, \theta)$ with $Z = \int p(\theta) \prod_{i=1}^{n} p(y_i | x_i, \theta) d\theta$
		\item \textit{Predictions:} $p(y^* | x^*, x_{1:n}, y_{1:n}) = \int p(y^*|x^*, \theta)p(\theta | x_{1:n}, y_{1:n}) d\theta$
	\end{itemize}
	
	\section*{Gaussian Processes}
	Given $\{x_1,\ldots,x_m\}$ and $y_i = f(x_i) + \epsilon_i$ with $\epsilon_i \sim \gauss(0,\sigma^2)$. Then $p(f|x_1,\ldots,x_m,y_1,\ldots,y_m) = GP(f;\mu',k')$ where:
	$\mu' = \mu(x) + k_{x,A}(K_{AA} + \sigma^2 I )^{-1}(y_A - \mu_A)$ and $k'(x,x') = k(x,x') - k_{x,A}(K_{AA} + \sigma^2 I)^{-1} K_{x,A}^T$ with $k_{x,A} = [k(x,x_1), \ldots, k(x,x_m)]$
	\subsection*{Kernels}
	Must be symmetric $k(x,x') = k(x',x)$ and positive semidefinite: $x^TKx \geq 0$, all Eigenvals $\geq 0$.
	\begin{itemize}
		\item Stationary $k(x,x') = k(x-x')$
		\item Isotropic $k(x,x') = k(|||x-x'||_2)$
		\item $k(x,x') = k_1(x,x') + k_2(x,x')$, $k(x,x') = k_1(x,x') \cdot k_2(x,x')$
		\item $k(x,x') = c\cdot k_1(x,x')$, $k(x,x') = f(k_1(x,x'))$ with $f$ $\exp$ or pos coeff poly.
	\end{itemize}
	We optimize over hyperparam by TODO \\
	How to speed up TODO
	
	\section*{Approximating Intractable Distributions}
	\textbf{Problem:} $Z = \int p(\theta) \prod_{i=1}^{n} p(y_i| x_i, \theta)$ is often intractable: Solutions: Variational Inference and MCMC.
	
	\section*{Variational Inference}
	Approximate $p(\theta|y) = \frac{p(\theta, y)}{Z}\approx q(\theta| \lambda)\equiv q_\lambda$ where $\lambda$ is variational parameter.
	\subsection*{Laplace Approximation}
	Gaussian approx of posterior using Taylor: 
	TODO
	\subsection*{Kullback-Leibler Divergence}
	$KL(q||p) = \int q(\theta) \log \frac{q(\theta)}{p(\theta)} d\theta$ with $KL(q||p) \geq 0$, generally not symmetric and zero iff agree almost everywhere.
	
	\subsection*{Deriving ELBO}
	\begin{align*}
		&\argmin_q \ KL(q||p) = \argmin \ \E[ \log \frac{q(\theta)}{\frac{1}{Z} p(\theta,y)} ]\\
	=& \argmin_q \  \E\left[ \log \ q(\theta) + \log \ Z - \log \ p(\theta,y) \right]\\
	=&  \argmax_q \ \E\left[ \log \ p(\theta,y) \right] + H(q)\\
	=& \argmax_q \E \left[ \log p(x| \theta) + \log p(\theta) - \log q(\theta) \right] \\
	&= \argmax_q \E \left[ \log p(x| \theta) \right] + KL(q||p(\cdot))
	\end{align*}
Where $\E = \E_{\theta \sim q}$ and $p(\cdot)$ at the end is the prior.
We call $\log p(y)$ the evidence

TODO


\section*{Markov Chains}

\subsection*{Hoeffding's Inequality}
Suppose $f$ is bounded in $\left[0,C\right]$ then $$P\left(| \E_P\left[f(X)\right] - \frac{1}{N} \sum_{i=1}^{N} f(x_i)| > \epsilon \right) \leq 2 \exp\left(-2N\epsilon^2/C^2\right)$$
\subsection*{Stationary Markov Chain}
If $\forall t$ we have $P(X_{t+1} | X_{t})$ independent of $t$.
\subsection*{Ergodic Markov Chain}
There exists a finite $t$ s.t. all states can be reached from every state in exactly $t$ steps.













\section*{Active Learning}
Which data to sample?
TODO

\section*{Bayesian Optimization}
\subsection*{Upper Confidence Sampling (GP-UCB)}
$x_{t+1} = \argmax_{x\in D} \mu_{t-1}(x) + \beta_t \sigma_{t-1} (x)$
\subsection*{Thompson Sampling}
$\tilde{f} \sim P(F|x_{1:t}, y_{1:t})$ and select $x_{t+1} \in \argmax_{x\in D} \tilde{f}(x)$
\section*{Markov Decision Processes}

\subsection*{Bellman Equations} TODO is this correct:
$V^\pi(x) = \sum_a \pi(a|x) (R(x,a) + \gamma \sum_{x'} P(x'|x,a) V^\pi (x'))$\\
$Q^\pi(x,a) = R(x,a) + \gamma \sum_{x'} P(x'|x,a) \sum_{a'} \pi(a'|x')Q^\pi(x',a')$
\subsection*{Bellman Optimality Equations}
$V^*(x) = \max_a R(x,a) + \gamma \sum_{x'} P(x' | x,y) V^*(x')$\\
$Q^*(x,a) = R(x,a) + \gamma \sum_{x'}P(x'|x,a) \max_{a'} Q^*(x',a')$
\subsection*{Policy Iteration}
Start with random policy, evaluate $\mathbf{V}^\pi = \mathbf{R}^\pi + \gamma \mathbf{P}^\pi \mathbf{V}^\pi$, the use Bellman Optimality Criterion select better policy.
\subsection*{Value Iteration}
Init $V_0(x)$, $Q_t(x,a) = r(x,a) + \gamma \sum_{x'} P(x'|x,a)V_{t-1}(x')$, $V_t(x) = \max_a Q_t (x,a)$, $t+=1$ until conv.
\subsection*{POMDP}
TODO

\section*{Reinforcment Learning}
\subsection*{Model-based RL}
Idea: Learn the MDP:
\begin{itemize}
	\item MLE: $P(X_{t+1}|X_t,A) = \frac{Cnt(X_{t+1},X_t,A)}{Cnt(X_t,A)}$, $r(x,a)=\frac{\sum_{t:X_t=x,A_t=a} R_t }{N_{x,a}}$
	\item TODO $R_{max}$
\end{itemize}

then optimize policy based on MDP



\subsection*{Model-free RL}
Either via Policy or Value (or Actor Critic)
\textbf{On-Policy:} $Q(x,a) \leftarrow Q(x,a) + \alpha (r + \gamma Q(x',\pi(x')) -Q(x,a) )$\\
\textbf{On-Policy:} $Q(x,a) \leftarrow Q(x,a) + \alpha (r + \gamma \max_{a'} Q(x',a') - Q(x,a) )$

TODO!!!! SARSA \& Q-learning.

\subsection*{Convergence}
If $\sum_t \alpha_t = \infty$ and $\sum_t \alpha_t^2 < \infty$ and all states visited infinitely often, then $\hat{V}^\pi$ converges to $V^\pi$ 


\subsection*{Policy Gradient Methods}
We want $\theta^* = \argmax_\theta J(\theta) $ with\\
 $J(\theta) = \E_{x_{0:T},a_{0:T} \sim \pi_\theta }\sum_{t=0}^{T} \gamma^t r(x_t,a_t) = \E_{\tau \sim \pi_\theta} r(\tau)$\\
 We have $\nabla J(\theta) = \nabla \E_{\tau \sim \pi_0}[r(\tau) \nabla \log \pi_\theta (\tau)]$ $\forall b$ for var red.\\
 $\nabla \E_{\tau \sim \pi_0}[r(\tau) \nabla \log \pi_\theta (\tau)] = \nabla \E_{\tau \sim \pi_0}[(r(\tau)-b) \nabla \log \pi_\theta (\tau)]$
 
\subsection*{REINFORCE (MC Policy Gradient)}
Init $\pi$ at rand. Gen. episode. Then for all timesteps $t$: $\theta \leftarrow \theta + \alpha \gamma^t G_t \nabla \log \pi (A_t|S_t,\theta)$.

\subsection*{Actor Critic}
Init $s,\pi,w$ at rand. Gen. episode. Then for all timesteps $t$: $\theta \leftarrow \theta + \alpha \gamma^t G_t \nabla \log \pi (A_t|S_t,\theta)$. TODO



\newpage

\section*{Useful math}
\subsection*{MAP and MLE}
\textbf{MLE} $\theta = \argmax_\theta P(X|\theta) = \argmax_\theta \prod_{i=1} P(x_i|\theta)$\\
\textbf{MAP} $\theta = \argmax_\theta P(X|\theta) P(\theta)$


\subsection*{Gaussian}
$\gauss(y;\Sigma,\mu) = \frac{1}{(2\pi)^{n/2} \sqrt{|\Sigma|}} \exp \left( - \frac{1}{2} (y-\mu)^T \Sigma^{-1}(y-\mu) \right) $ where $\Sigma$ is the covariance matrix.

Let X be a random variable with $X_V \sim \gauss(\mu_V,\Sigma_{VV})$ then we have:
\begin{itemize}
	\item Marginal distribution of only variables indexed by $A$: $X_A \sim \gauss(\mu_A,\Sigma_{AA})$ (we drop all marginalized ones)
	\item Conditioning on B gives $p(X_A,X_B = x_B) = \gauss(\mu_{A|B},\Sigma_{A|B})$ where we have $\mu_{A|B} = \mu_A + \Sigma_{AB}\Sigma_{BB}^{-1}(X_B - \mu_B)$ and $\Sigma_{A|B} = \Sigma_{AA} - \Sigma_{AB}\Sigma_{BB}^{-1}\Sigma_{BA}$.
	\item Multiples of Gaussians. Assume $M\in \mathbb{R}^{m\times d}$ then for $Y=MX$ we have $Y\sim \gauss (M\mu_V,M\Sigma_{VV}M^T)$.
	\item Sums of Gaussians $X+X'=Y$ yields $Y\sim \gauss (\mu_V + \mu_{V'} ,\Sigma_{VV} + \Sigma^{'}_{VV})$.
\end{itemize}


\subsection*{Probabilities}
$\mathbb{E}_x[X] = \begin{cases}
	\int x \cdot p(x) \partial x  & \text{if continuous}\\
	\sum_x x \cdot p(x) & \text{otherwise}
\end{cases}$\\
$\operatorname{Var}[X] = \mathbb{E}[(X-\mu_X)^2] = \mathbb{E}[X^2] - \mathbb{E}[X]^2$\\
$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$; $p(Z|X,\theta) = \frac{p(X,Z|\theta)}{p(X|\theta)}$\\
$P(x,y) = P(y|x) \cdot  P(x) = P(x|y) \cdot P(y)$

\subsection*{{Bayes Rule}}
$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$

\subsection*{P-Norm}
$||x||_p = (\sum_{i=1}^n|x_i|^p)^{\frac{1}{p}}$, $1 \leq p < \infty$

\subsection*{Some gradients}
$\nabla_x ||x||_2^2 = 2 x$\\
$f(x) = x^T A x$; $\nabla_x f(x) = (A + A^T) x$\\
E.g. $\nabla_w \operatorname{log}(1+\operatorname{exp(-y w^T x)}) = \\
\frac{1}{1+\operatorname{exp}(-y w^T x)} \cdot \operatorname{exp}(-y w^T x) \cdot (-y x) = \\
\frac{1}{1 + \operatorname{exp}(y w^T x)} \cdot(-yx)$\\


\subsection*{Convex / Jensen's inequality}
$\text{g(x) convex} \Leftrightarrow g''(x) > 0 \Leftrightarrow x_1,x_2 \in \mathbb{R}, \lambda \in [0,1]: 
g(\lambda x_1 + (1-\lambda) x_2) \leq \lambda g(x_1) + (1-\lambda) g(x_2)$

\subsection*{Gaussian / Normal Distribution}
$f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} exp(-\frac{(x-\mu)^2}{2\sigma^2})$

\subsection*{Multivariate Gaussian}
$\Sigma =$ covariance matrix, $\mu$ = mean\\
$f(x) = \frac{1}{2\pi \sqrt{|\Sigma|}} e^{- \frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu)}$\\
Empirical: $\hat{\Sigma} = \frac{1}{n}\sum_{i=1}^n x_i x_i^T$ (needs centered data points)

\subsection*{Positive semi-definite matrices}
$M \in \mathbb{R}^{n\times n}$ is psd $\Leftrightarrow$\\
$\forall x \in \mathbb{R}^n: x^TMx \geq 0 \Leftrightarrow$\\
all eigenvalues of $M$ are positive: $\lambda_i\geq 0$

TODO distribution stuff!!!!	
\end{multicols*}




	
\end{document}
